import argparse

parser = argparse.ArgumentParser("Hyperparameter Setting for PPO-discrete")
parser.add_argument("--max_train_steps", type=int, default=int(1e4), help=" Maximum number of training steps")  # 2e5
parser.add_argument("--evaluate_freq", type=float, default=5e3, help="Evaluate the policy every 'evaluate_freq' steps")
parser.add_argument("--save_freq", type=int, default=1000, help="Save frequency")
parser.add_argument("--batch_size", type=int, default=64, help="Batch size")
parser.add_argument("--mini_batch_size", type=int, default=64, help="Minibatch size")
parser.add_argument("--state_dim", type=int, default=32, help="The dimension of local state")
parser.add_argument("--task_state_dim", type=int, default=5, help="The dimension of task")
parser.add_argument("--num_input", type=int, default=5, help="The types of input")
parser.add_argument("--machine_state_dim", type=int, default=6, help="The dimension of machine")
parser.add_argument("--action_state_dim", type=int, default=3, help="The dimension of action")
parser.add_argument("--global_state_dim", type=int, default=24, help="The dimension of global state")
parser.add_argument("--num_hidden_layer", type=int, default=3, help="The number of hidden layer")
parser.add_argument("--hidden_width", type=int, default=10, help="The number of neurons in hidden layers of the neural network")
parser.add_argument("--task_hidden_width", type=int, default=64,
                        help="The number of neurons in hidden layers of the neural network")
parser.add_argument("--actor_hidden_layers", type=int, default=5, help="The number of hidden layers in the actor policy network")
parser.add_argument("--critic_hidden_layers", type=int, default=3, help="The number of hidden layers in the critic policy network")
parser.add_argument("--lr_a", type=float, default=3e-4, help="Learning rate of actor")
parser.add_argument("--lr_c", type=float, default=3e-4, help="Learning rate of critic")
parser.add_argument("--global_lr_a", type=float, default=1e-4, help="Learning rate of actor in controller agent")
parser.add_argument("--global_lr_c", type=float, default=1e-4, help="Learning rate of critic in controller agent")
parser.add_argument("--gamma", type=float, default=0.99, help="Discount factor")
parser.add_argument("--lamda", type=float, default=0.95, help="GAE parameter")
parser.add_argument("--epsilon", type=float, default=0.2, help="PPO clip parameter")
parser.add_argument("--K_epochs", type=int, default=10, help="PPO parameter")
parser.add_argument("--save_dir", type=str, default='E:/Phd_work/myWork/Code/2/trained_models/', help="Save path of the model")
# parser.add_argument("--save_control_dir", type=str, default='E:/Phd_work/myWork/Code/2/trained_models/', help="Save path of the model")

parser.add_argument("--num_tasks", type=int, default=3, help="The number of tasks")
parser.add_argument("--num_total_jobs", type=int, default=20, help="The number of jobs in system")
parser.add_argument("--num_new_jobs", type=int, default=100, help="The number of new jobs")
parser.add_argument("--num_machines", type=int, default=10, help="The number of machines")
parser.add_argument("--n_action", type=int, default=8, help="The number of dispatching rules")
parser.add_argument("--E_utliz", type=float, default=0.9, help="The machine utilization ")
parser.add_argument("--num_ops_range", type=tuple, default=(1, 10), help="The range of operations in a job")
parser.add_argument("--num_cand_machines_range", type=tuple, default=(1, 10),
                    help="The range of candidate machines for each operation")
parser.add_argument("--weights", type=list, default=[0.2, 0.6, 0.2], help="The weight of each job")
parser.add_argument("--processing_time_range", type=tuple, default=(1, 99),
                    help="The processing time of an operation")
parser.add_argument("--due_time_multiplier", type=float, default=1.5, help="The due time multiplier of a job")
parser.add_argument("--num_warmup_jobs", type=int, default=10, help="The number of warmup jobs")
parser.add_argument("--seed", type=int, default=123, help="seed")

parser.add_argument("--use_adv_norm", type=bool, default=True, help="Trick 1:advantage normalization")
parser.add_argument("--use_state_norm", type=bool, default=True, help="Trick 2:state normalization")
parser.add_argument("--use_reward_norm", type=bool, default=False, help="Trick 3:reward normalization")
parser.add_argument("--use_reward_scaling", type=bool, default=False, help="Trick 4:reward scaling")
parser.add_argument("--entropy_coef", type=float, default=0.01, help="Trick 5: policy entropy")
parser.add_argument("--use_lr_decay", type=bool, default=True, help="Trick 6:learning rate Decay")
parser.add_argument("--use_grad_clip", type=bool, default=True, help="Trick 7: Gradient clip")
parser.add_argument("--use_orthogonal_init", type=bool, default=True, help="Trick 8: orthogonal initialization")
parser.add_argument("--set_adam_eps", type=float, default=True, help="Trick 9: set Adam epsilon=1e-5")
parser.add_argument("--use_tanh", type=float, default=False, help="Trick 10: tanh activation function")

args = parser.parse_args()
